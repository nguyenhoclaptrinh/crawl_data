import concurrent.futures
import json
import requests
from requests.exceptions import RequestException
from bs4 import BeautifulSoup
import urllib3
import time
import os
from crawl_url_pdf import download_pdf
from config import (BASE_URL, BASE_DOMAIN, DATASET_DIR, CHECKPOINT_DIR,
                    DEFAULT_MAX_PAGES, DEFAULT_BATCH_SIZE, MIN_BATCH_SIZE, MAX_BATCH_SIZE,
                    DROP_LEVELS_OPTIONS, DEFAULT_DROP_LEVELS, SEARCH_KEYWORD)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def get_hidden_fields(response):
    try:
        soup = BeautifulSoup(response, "html.parser")

        hidden = {}

        for input_tag in soup.find_all("input", type="hidden"):
            if input_tag.get("name") and input_tag.get("value"):
                hidden[input_tag["name"]] = input_tag["value"]

        return hidden
    except RequestException as e:
        print(f"Error fetching hidden fields: {e}")
        return {}


def initialize_session():
    """Kh·ªüi t·∫°o session v√† l·∫•y hidden fields ban ƒë·∫ßu"""
    session = requests.Session()
    response = session.get(BASE_URL, verify=False)
    response.raise_for_status()
    hidden_fields = get_hidden_fields(response.text)
    return session, hidden_fields


def get_user_configuration():
    """Cho ph√©p user nh·∫≠p maxpages v√† t·ª± ƒë·ªông t√≠nh to√°n batch configuration"""
    print("\n" + "="*60)
    print("‚öôÔ∏è  C·∫§U H√åNH CRAWL DATA")
    print("="*60)

    # Nh·∫≠p s·ªë pages t·ªëi ƒëa
    while True:
        try:
            max_pages_input = input(
                f"üìÑ Nh·∫≠p s·ªë pages t·ªëi ƒëa ƒë·ªÉ crawl (m·∫∑c ƒë·ªãnh {DEFAULT_MAX_PAGES}): ").strip()
            if not max_pages_input:
                max_pages = DEFAULT_MAX_PAGES
            else:
                max_pages = int(max_pages_input)
                if max_pages <= 0:
                    print("‚ùå S·ªë pages ph·∫£i l·ªõn h∆°n 0")
                    continue
            break
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p s·ªë nguy√™n h·ª£p l·ªá")

    # T·ª± ƒë·ªông t√≠nh to√°n batch size t·ªëi ∆∞u ho·∫∑c cho user ch·ªçn
    print(f"\nüìä V·ªõi {max_pages} pages, c√°c t√πy ch·ªçn batch size:")

    # T√≠nh to√°n c√°c t√πy ch·ªçn batch size h·ª£p l√Ω
    batch_options = calculate_batch_options(max_pages)

    for i, option in enumerate(batch_options):
        batch_size = option["batch_size"]
        num_batches = option["num_batches"]
        efficiency = option["efficiency"]
        print(
            f"  [{i+1}] Batch size {batch_size}: {num_batches} batches (hi·ªáu qu·∫£: {efficiency:.1f}%)")

    print(f"  [0] T·ª± nh·∫≠p batch size")

    # User ch·ªçn batch size
    while True:
        try:
            choice = input(
                f"\nCh·ªçn t√πy ch·ªçn (1-{len(batch_options)} ho·∫∑c 0): ").strip()
            if not choice:
                # M·∫∑c ƒë·ªãnh ch·ªçn t√πy ch·ªçn ƒë·∫ßu ti√™n (t·ªëi ∆∞u nh·∫•t)
                chosen_batch_size = batch_options[0]["batch_size"]
                break

            choice_num = int(choice)
            if choice_num == 0:
                chosen_batch_size = get_custom_batch_size(max_pages)
                break
            elif 1 <= choice_num <= len(batch_options):
                chosen_batch_size = batch_options[choice_num - 1]["batch_size"]
                break
            else:
                print(f"‚ùå Vui l√≤ng ch·ªçn t·ª´ 0 ƒë·∫øn {len(batch_options)}")
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p s·ªë nguy√™n h·ª£p l·ªá")

    # T√≠nh to√°n s·ªë batches
    num_batches = calculate_num_batches(max_pages, chosen_batch_size)

    print(f"\n‚úÖ C·∫•u h√¨nh ƒë√£ ch·ªçn:")
    print(f"   üìÑ Max pages: {max_pages}")
    print(f"   üì¶ Batch size: {chosen_batch_size}")
    print(f"   üî¢ S·ªë batches: {num_batches}")
    print(
        f"   üìä Pages trong batch cu·ªëi: {max_pages % chosen_batch_size if max_pages % chosen_batch_size != 0 else chosen_batch_size}")

    return max_pages, chosen_batch_size, num_batches


def calculate_batch_options(max_pages):
    """T√≠nh to√°n c√°c t√πy ch·ªçn batch size h·ª£p l√Ω"""
    options = []

    # Th·ª≠ c√°c batch size t·ª´ MIN ƒë·∫øn MAX
    for batch_size in range(MIN_BATCH_SIZE, min(MAX_BATCH_SIZE, max_pages) + 1):
        num_batches = calculate_num_batches(max_pages, batch_size)

        # T√≠nh hi·ªáu qu·∫£ (% pages ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·∫ßy ƒë·ªß)
        full_batches = max_pages // batch_size
        remaining_pages = max_pages % batch_size
        if remaining_pages == 0:
            efficiency = 100.0
        else:
            efficiency = (full_batches * batch_size +
                          remaining_pages) / (num_batches * batch_size) * 100

        options.append({
            "batch_size": batch_size,
            "num_batches": num_batches,
            "efficiency": efficiency
        })

    # S·∫Øp x·∫øp theo hi·ªáu qu·∫£ gi·∫£m d·∫ßn, r·ªìi theo batch size tƒÉng d·∫ßn
    options.sort(key=lambda x: (-x["efficiency"], x["batch_size"]))

    # Ch·ªâ tr·∫£ v·ªÅ 5 t√πy ch·ªçn t·ªët nh·∫•t
    return options[:5]


def get_custom_batch_size(max_pages):
    """Cho ph√©p user nh·∫≠p batch size t√πy ch·ªânh"""
    while True:
        try:
            batch_size = int(input(
                f"üì¶ Nh·∫≠p batch size ({MIN_BATCH_SIZE}-{min(MAX_BATCH_SIZE, max_pages)}): "))
            if MIN_BATCH_SIZE <= batch_size <= min(MAX_BATCH_SIZE, max_pages):
                return batch_size
            else:
                print(
                    f"‚ùå Batch size ph·∫£i t·ª´ {MIN_BATCH_SIZE} ƒë·∫øn {min(MAX_BATCH_SIZE, max_pages)}")
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p s·ªë nguy√™n h·ª£p l·ªá")


def calculate_num_batches(max_pages, batch_size):
    """T√≠nh s·ªë batches c·∫ßn thi·∫øt"""
    return (max_pages + batch_size - 1) // batch_size  # Ceiling division


def get_batch_page_range(batch_num, batch_size, max_pages):
    """T√≠nh to√°n start_page v√† end_page cho m·ªôt batch c·ª• th·ªÉ"""
    start_page = (batch_num - 1) * batch_size + 1
    end_page = min(batch_num * batch_size, max_pages)
    return start_page, end_page


def get_checkpoint_filename(drop_levels, batch_num):
    """T·∫°o t√™n file checkpoint theo DROP_LEVELS v√† batch number"""
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p DROP_LEVELS r·ªóng
    level_code = drop_levels if drop_levels else "ALL"
    return f"checkpoint_{level_code}_{batch_num}.json"


def get_checkpoint_filepath(drop_levels, batch_num):
    """L·∫•y ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß c·ªßa file checkpoint"""
    if not os.path.exists(CHECKPOINT_DIR):
        os.makedirs(CHECKPOINT_DIR)

    filename = get_checkpoint_filename(drop_levels, batch_num)
    return os.path.join(CHECKPOINT_DIR, filename)


def create_checkpoint_structure(drop_levels, batch_num, start_page, end_page, max_pages, batch_size):
    """T·∫°o c·∫•u tr√∫c checkpoint m·ªõi v·ªõi th√¥ng tin batch configuration"""
    return {
        "drop_levels": drop_levels,
        "drop_levels_name": DROP_LEVELS_OPTIONS.get(drop_levels, f"C·∫•p {drop_levels}"),
        "batch_number": batch_num,
        "batch_size": batch_size,
        "max_pages": max_pages,
        "start_page": start_page,
        "end_page": end_page,
        "last_processed_page": 0,  # Page cu·ªëi c√πng ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng
        "total_links_found": 0,
        "total_pdfs_downloaded": 0,
        "failed_pages": [],
        "completed_pages": [],
        "created_at": time.time(),
        "last_updated": time.time(),
        "is_completed": False
    }


def load_checkpoint(drop_levels, batch_num):
    """T·∫£i checkpoint t·ª´ file c·ª• th·ªÉ"""
    filepath = get_checkpoint_filepath(drop_levels, batch_num)

    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            checkpoint_data = json.load(f)
            print(
                f"‚úÖ Loaded checkpoint: {get_checkpoint_filename(drop_levels, batch_num)}")
            print(
                f"   üìÑ Pages: {checkpoint_data['start_page']}-{checkpoint_data['end_page']}")
            print(
                f"   ‚úèÔ∏è  Last processed: {checkpoint_data['last_processed_page']}")
            print(f"   üîó Links found: {checkpoint_data['total_links_found']}")
            print(
                f"   üì• PDFs downloaded: {checkpoint_data['total_pdfs_downloaded']}")
            return checkpoint_data
    else:
        print(
            f"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint: {get_checkpoint_filename(drop_levels, batch_num)}")
        return None


def save_checkpoint(checkpoint_data):
    """L∆∞u checkpoint v√†o file"""
    checkpoint_data["last_updated"] = time.time()

    filepath = get_checkpoint_filepath(
        checkpoint_data["drop_levels"],
        checkpoint_data["batch_number"]
    )

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)

    filename = get_checkpoint_filename(
        checkpoint_data["drop_levels"],
        checkpoint_data["batch_number"]
    )
    print(f"üíæ Saved checkpoint: {filename}")


def update_checkpoint_progress(checkpoint_data, page_num, links_found, success=True):
    """C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô x·ª≠ l√Ω page"""
    if success:
        if page_num not in checkpoint_data["completed_pages"]:
            checkpoint_data["completed_pages"].append(page_num)
            checkpoint_data["total_links_found"] += links_found

        # C·∫≠p nh·∫≠t last_processed_page
        if page_num > checkpoint_data["last_processed_page"]:
            checkpoint_data["last_processed_page"] = page_num

        # X√≥a kh·ªèi failed_pages n·∫øu c√≥
        if page_num in checkpoint_data["failed_pages"]:
            checkpoint_data["failed_pages"].remove(page_num)
    else:
        if page_num not in checkpoint_data["failed_pages"]:
            checkpoint_data["failed_pages"].append(page_num)

    # Ki·ªÉm tra xem batch ƒë√£ ho√†n th√†nh ch∆∞a
    if checkpoint_data["last_processed_page"] >= checkpoint_data["end_page"]:
        checkpoint_data["is_completed"] = True

    return checkpoint_data


def list_all_checkpoints():
    """Li·ªát k√™ t·∫•t c·∫£ checkpoint files c√≥ s·∫µn"""
    if not os.path.exists(CHECKPOINT_DIR):
        return []

    checkpoint_files = []
    for filename in os.listdir(CHECKPOINT_DIR):
        if filename.startswith("checkpoint_") and filename.endswith(".json"):
            # Parse filename: checkpoint_{drop_levels}_{batch}.json
            parts = filename.replace("checkpoint_", "").replace(
                ".json", "").split("_")
            if len(parts) >= 2:
                drop_levels = parts[0] if parts[0] != "ALL" else ""
                try:
                    batch_num = int(parts[1])
                    checkpoint_files.append({
                        "filename": filename,
                        "drop_levels": drop_levels,
                        "batch_number": batch_num,
                        "filepath": os.path.join(CHECKPOINT_DIR, filename)
                    })
                except ValueError:
                    continue

    return sorted(checkpoint_files, key=lambda x: (x["drop_levels"], x["batch_number"]))


def display_checkpoint_status_and_choose(max_pages, batch_size, num_batches):
    """Hi·ªÉn th·ªã tr·∫°ng th√°i t·∫•t c·∫£ checkpoint v√† cho ph√©p user ch·ªçn"""
    print("\n" + "="*80)
    print("üìä H·ªÜ TH·ªêNG CHECKPOINT THEO DROP_LEVELS + BATCH")
    print("="*80)
    print(
        f"üìÑ Configuration: {max_pages} pages, batch size {batch_size}, {num_batches} batches")

    # Hi·ªÉn th·ªã c√°c DROP_LEVELS c√≥ s·∫µn
    print("\nüéØ C√°c c·∫•p t√≤a √°n:")
    for key, name in DROP_LEVELS_OPTIONS.items():
        print(f"  [{key}] {name}")

    # Li·ªát k√™ t·∫•t c·∫£ checkpoint hi·ªán c√≥
    checkpoints = list_all_checkpoints()
    if checkpoints:
        print(f"\nüìã Checkpoint hi·ªán c√≥ ({len(checkpoints)} files):")
        for ckpt in checkpoints:
            # Load chi ti·∫øt checkpoint
            data = load_checkpoint(ckpt["drop_levels"], ckpt["batch_number"])
            if data:
                status_icon = "‚úÖ" if data["is_completed"] else "‚è≥"
                progress = f"{data['last_processed_page']}/{data['end_page']}"
                batch_info = f"Batch {data['batch_number']}"
                if "batch_size" in data:
                    batch_info += f" (size {data['batch_size']})"

                print(f"  {status_icon} {ckpt['filename']}: "
                      f"{batch_info}, Pages {progress}, "
                      f"{data['total_links_found']} links, "
                      f"{data['total_pdfs_downloaded']} PDFs")

                if data["failed_pages"]:
                    print(f"      ‚ùå Failed pages: {data['failed_pages']}")
    else:
        print("\nüìã Ch∆∞a c√≥ checkpoint n√†o.")

    print("\n" + "-"*50)
    print("üéØ Ch·ªçn c√¥ng vi·ªác:")
    print("  1. T·∫°o batch m·ªõi")
    print("  2. Ti·∫øp t·ª•c batch ƒë√£ c√≥")

    choice = input("Nh·∫≠p l·ª±a ch·ªçn (1/2): ").strip()

    if choice == "1":
        return choose_new_batch(max_pages, batch_size, num_batches)
    elif choice == "2":
        result = choose_existing_batch(checkpoints)
        if result is None:
            print("üîÑ Chuy·ªÉn sang t·∫°o batch m·ªõi...")
            return choose_new_batch(max_pages, batch_size, num_batches)
        return result
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh: t·∫°o batch m·ªõi")
        return choose_new_batch(max_pages, batch_size, num_batches)


def choose_new_batch(max_pages, batch_size, num_batches):
    """Cho ph√©p user ch·ªçn DROP_LEVELS v√† batch ƒë·ªÉ t·∫°o m·ªõi"""
    print("\nüÜï T·∫†O BATCH M·ªöI")
    print("-" * 30)
    # Ch·ªâ cho ph√©p user ch·ªçn DROP_LEVELS
    drop_levels = input(
        f"Ch·ªçn c·∫•p t√≤a √°n (TW/CW/T/H ho·∫∑c Enter cho {DEFAULT_DROP_LEVELS}): ").strip().upper()
    if not drop_levels:
        drop_levels = DEFAULT_DROP_LEVELS
    if drop_levels not in DROP_LEVELS_OPTIONS:
        print(f"‚ùå C·∫•p '{drop_levels}' kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh '{DEFAULT_DROP_LEVELS}'")
        drop_levels = DEFAULT_DROP_LEVELS
    print(f"\n‚úÖ S·∫Ω t·∫°o batch ƒëa lu·ªìng cho c·∫•p t√≤a √°n: {drop_levels} ({DROP_LEVELS_OPTIONS[drop_levels]})")
    return drop_levels


def choose_existing_batch(checkpoints):
    """Cho ph√©p user ch·ªçn batch ƒë√£ c√≥ ƒë·ªÉ ti·∫øp t·ª•c"""
    if not checkpoints:
        print("‚ùå Kh√¥ng c√≥ checkpoint n√†o ƒë·ªÉ ti·∫øp t·ª•c")
        # Kh√¥ng th·ªÉ g·ªçi choose_new_batch() v√¨ thi·∫øu tham s·ªë, return None ƒë·ªÉ main x·ª≠ l√Ω
        return None

    print(f"\nüîÑ TI·∫æP T·ª§C BATCH ƒê√É C√ì")
    print("-" * 30)

    incomplete_checkpoints = []
    for i, ckpt in enumerate(checkpoints):
        data = load_checkpoint(ckpt["drop_levels"], ckpt["batch_number"])
        if data and not data["is_completed"]:
            incomplete_checkpoints.append((i, ckpt, data))
            print(
                f"  [{len(incomplete_checkpoints)}] {ckpt['filename']}: Pages {data['last_processed_page']}/{data['end_page']}")

    if not incomplete_checkpoints:
        print("‚ùå Kh√¥ng c√≥ checkpoint n√†o ch∆∞a ho√†n th√†nh")
        return None

    try:
        choice_idx = int(
            input("Ch·ªçn checkpoint ƒë·ªÉ ti·∫øp t·ª•c (s·ªë th·ª© t·ª±): ")) - 1
        if 0 <= choice_idx < len(incomplete_checkpoints):
            original_idx, ckpt, data = incomplete_checkpoints[choice_idx]

            return (ckpt["drop_levels"], ckpt["batch_number"],
                    data["start_page"], data["end_page"], data)
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
            return None
    except ValueError:
        print("‚ùå Vui l√≤ng nh·∫≠p s·ªë")
        return None


def create_payload(hidden_fields, page, drop_levels):
    """T·∫°o payload cho request t√πy theo page s·ªë v√† drop_levels"""
    if page == 1:
        # L·∫ßn ƒë·∫ßu: b·∫•m n√∫t "T√¨m ki·∫øm"
        return {
            **hidden_fields,
            "ctl00$Content_home_Public$ctl00$txtKeyword": SEARCH_KEYWORD,
            "ctl00$Content_home_Public$ctl00$Drop_Levels": drop_levels,
            "ctl00$Content_home_Public$ctl00$Ra_Drop_Courts": "",
            "ctl00$Content_home_Public$ctl00$Rad_DATE_FROM": "",
            "ctl00$Content_home_Public$ctl00$cmd_search_banner": "T√¨m ki·∫øm"
        }
    else:
        return {
            **hidden_fields,
            "ctl00$Content_home_Public$ctl00$txtKeyword": SEARCH_KEYWORD,
            "ctl00$Content_home_Public$ctl00$Drop_Levels": drop_levels,
            "ctl00$Content_home_Public$ctl00$Ra_Drop_Courts": "",
            "ctl00$Content_home_Public$ctl00$Rad_DATE_FROM": "",
            "ctl00$Content_home_Public$ctl00$DropPages": str(page),
            "__EVENTTARGET": "ctl00$Content_home_Public$ctl00$DropPages",
            "__EVENTARGUMENT": ""
        }


def crawl_page(session, page, hidden_fields, drop_levels):
    """Crawl m·ªôt page v√† tr·∫£ v·ªÅ danh s√°ch links + hidden_fields m·ªõi"""
    payload = create_payload(hidden_fields, page, drop_levels)

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Content-Type": "application/x-www-form-urlencoded"
    }

    try:
        response = session.post(BASE_URL, data=payload,
                                headers=headers, verify=False)
        response.raise_for_status()

        print(
            f"üìÑ Page {page} (DROP_LEVELS={drop_levels}) fetched successfully.")

        soup = BeautifulSoup(response.text, "html.parser")

        links = [a["href"] for a in soup.find_all("a", href=True)]
        page_links = []

        for link in links:
            text_split = link.split("/")
            if len(text_split) > 2 and text_split[2] == "chi-tiet-ban-an":
                full_link = BASE_DOMAIN + link
                page_links.append(full_link)

        new_hidden_fields = get_hidden_fields(response.text)
        print(f"‚úÖ Found {len(page_links)} detail links on page {page}")
        return page_links, new_hidden_fields, True

    except RequestException as e:
        print(f"‚ùå Error on page {page}: {e}")
        return [], hidden_fields, False


def process_and_deduplicate_links(all_links):
    """X·ª≠ l√Ω v√† lo·∫°i b·ªè duplicate links"""
    set_all_links = set(all_links)
    list_all_links = list(set_all_links)
    print(f"Total unique links collected: {len(list_all_links)}")
    return list_all_links


def download_all_pdfs(links, session):
    """Download t·∫•t c·∫£ PDF t·ª´ danh s√°ch links"""
    if not os.path.exists(DATASET_DIR):
        os.makedirs(DATASET_DIR)

    for i, link in enumerate(links):
        print(f"{i+1}: {link}")
        download_pdf(link, session)


# --- ƒêA LU·ªíNG: m·ªói batch l√† 1 lu·ªìng ƒë·ªôc l·∫≠p ---


def batch_worker(drop_levels, batch_num, start_page, end_page, max_pages, batch_size, existing_checkpoint=None):
    print(
        f"\nüöÄ [BATCH {batch_num}] B·∫Øt ƒë·∫ßu t·ª´ page {start_page} ƒë·∫øn {end_page} (DROP_LEVELS={drop_levels})")
    session, hidden_fields = initialize_session()
    if existing_checkpoint:
        checkpoint_data = existing_checkpoint
        print(
            f"[BATCH {batch_num}] üîÑ Ti·∫øp t·ª•c t·ª´ page {checkpoint_data['last_processed_page'] + 1}")
        start_from_page = checkpoint_data['last_processed_page'] + 1
    else:
        checkpoint_data = create_checkpoint_structure(
            drop_levels, batch_num, start_page, end_page, max_pages, batch_size)
        save_checkpoint(checkpoint_data)
        print(
            f"[BATCH {batch_num}] üÜï T·∫°o checkpoint m·ªõi: {get_checkpoint_filename(drop_levels, batch_num)}")
        start_from_page = start_page

    for page in range(start_from_page, end_page + 1):
        print(f"[BATCH {batch_num}] --- Processing Page {page}/{end_page} ---")
        page_links, hidden_fields, success = crawl_page(
            session, page, hidden_fields, drop_levels)
        checkpoint_data = update_checkpoint_progress(
            checkpoint_data, page, len(page_links), success)
        if success:
            print(
                f"[BATCH {batch_num}] ‚úÖ Page {page} ho√†n th√†nh: {len(page_links)} links")
            for i, link in enumerate(page_links):
                print(
                    f"[BATCH {batch_num}]    üìÑ Downloading PDF {i+1}/{len(page_links)}: {link}")
                try:
                    download_pdf(link, session)
                    checkpoint_data["total_pdfs_downloaded"] += 1
                except Exception as e:
                    print(f"[BATCH {batch_num}]    ‚ùå L·ªói download: {e}")
        else:
            print(f"[BATCH {batch_num}] ‚ùå Page {page} th·∫•t b·∫°i")
        save_checkpoint(checkpoint_data)
        progress_percent = ((page - start_page + 1) /
                            (end_page - start_page + 1)) * 100
        print(
            f"[BATCH {batch_num}] üìà Progress: {progress_percent:.1f}% ({page - start_page + 1}/{end_page - start_page + 1} pages)")
        time.sleep(1)
    checkpoint_data["is_completed"] = True
    save_checkpoint(checkpoint_data)
    print(f"[BATCH {batch_num}] üéâ HO√ÄN TH√ÄNH!")


def main():
    print("üöÄ CRAWL D·ªÆ LI·ªÜU B·∫¢N √ÅN - ƒêA LU·ªíNG THEO BATCH")
    max_pages, batch_size, num_batches = get_user_configuration()

    # Gom c√°c batch th√†nh danh s√°ch
    batch_jobs = []
    for batch_num in range(1, num_batches + 1):
        start_page, end_page = get_batch_page_range(
            batch_num, batch_size, max_pages)
        batch_jobs.append({
            "batch_num": batch_num,
            "start_page": start_page,
            "end_page": end_page
        })

    # Hi·ªÉn th·ªã tr·∫°ng th√°i checkpoint v√† cho user ch·ªçn DROP_LEVELS
    drop_levels = display_checkpoint_status_and_choose(
        max_pages, batch_size, num_batches)

    # Chu·∫©n b·ªã checkpoint cho t·ª´ng batch (n·∫øu c√≥)
    checkpoints = list_all_checkpoints()
    batch_ckpt_map = {}
    for ckpt in checkpoints:
        if ckpt["drop_levels"] == drop_levels:
            batch_ckpt_map[ckpt["batch_number"]] = load_checkpoint(
                drop_levels, ckpt["batch_number"])

    # S·ªë lu·ªìng t·ªëi ƒëa l√† 10 ho·∫∑c s·ªë batch
    max_workers = min(10, num_batches)
    print(f"\nüßµ S·ª≠ d·ª•ng t·ªëi ƒëa {max_workers} lu·ªìng song song!")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for job in batch_jobs:
            batch_num = job["batch_num"]
            start_page = job["start_page"]
            end_page = job["end_page"]
            existing_ckpt = batch_ckpt_map.get(batch_num)
            futures.append(executor.submit(
                batch_worker,
                drop_levels,
                batch_num,
                start_page,
                end_page,
                max_pages,
                batch_size,
                existing_ckpt
            ))
        # ƒê·ª£i t·∫•t c·∫£ batch ho√†n th√†nh
        for future in concurrent.futures.as_completed(futures):
            future.result()

    print("\nüéâ T·∫§T C·∫¢ BATCH ƒê√É HO√ÄN TH√ÄNH!")


if __name__ == "__main__":
    main()
